#!/bin/env python3

import gzip
from collections import defaultdict
from functools import reduce

import networkx as nx
import pickle as pkl


## thoughts
## - write generator over read ids
## - add attributes for each
## - write fn to check if read is het/hom
## => VCFs as global state or fn args?
## => figure out vcf parsing => pysam?

def load_pickle(filepath) -> Dict:
    ret = None
    with open(filepath, 'rb') as f:
        ret = pickle.load(f)
    return ret

def save_pickle(obj, filepath):
    with open(filepath, 'wb') as f:
        ret = pickle.dump(obj, f)

#TODO make call to handle het/homozygous
# set zygosity mapping as global variable?
def check_zygosity(chrid, start, end):
    return 'E' # for now, everything is hEterozygous


def iter_badreads_fastq(inpath, gzipped=True):
    """
    Takes a fastq file produced by badreads and emits the headers containing the ground truth values as a dict to be added to the newtorkx graph along with each read ID
    """
    # handle gzipped/uncompressed files
    with gzip.open(inpath, "rt") if gzipped else open(inpath, "rt") as file:
        for line in file:
            # only parse headers
            if line[0] != '@':
                continue
            header = line.split(' ')[1:]
            assert len(header) == 4
            # header looks like
            # chr_id,+strand,start-end, length=x, error-free_legnth=x, read_identity=xx.yyy%
            pos = header[0].split(',')
            assert len(pos) == 3
            chr_id = header[0][0]
            strand = header[0][1][0] # should be always + or -
            assert strand == '+' or strand == '-'
            start, end = int(pos[2].split('-')[0]), int(pos[2].split('-')[1])
            assert start <= end and start >= 0 and end >= 0

            yield (idx, {'chr': chr_id, 'strand': strand, 'start': start, 'end': end})

### Example file
# a
# s ref   START   LEN +   END ACCTA
# s SXX_YY   START   LEN +   END ACCTA
#
# a
# s ref   START   LEN +   END ACCTA
# s SXX_YY   START   LEN +   END ACCTA

def iter_pbsim_maf(inpath, chrlist, gzipped=True):
    """
    Iterate through reads in a .maf file generated by concatenating the different output files of PBSim.
    Requires a list of chromosome IDs by occurence to map PBSim read IDs to chromosomes
    """

    with gzip.open(inpath, "rt") if gzipped else open(inpath, "rt") as file:
        lineiter = iter(file)
        
        for line in lineiter:
            # parse in bunches of three lines, starting with an 'a' line
            if line[0] == 'a':
                # store both lines as metadata array, skip sequence and initial s
                refline, readline = next(lineiter).split('\t')[1:-1], next(lineiter).split('\t')[1:-1]
                idx = readline[0]
                start = int(refline[1])
                end = start + int(seqline[2]) # doesn't seem to store end directly? => add len
                strand = seqline[3]
                # chr corresponds to no in ID
                chr_id = chrlist[int(idx[1:].split('_')[0])]

                yield (idx, {'chr': chr_id, 'strand': strand, 'start': start, 'end': end})
            elif line:
                print("ERROR: Non-empty line encountered without preceding 'a' line")


def main(inpath, nx_path, read_to_node_dict_path, outpath, badreads=True):
    graph = load_pickle(nx_path)
    r2n = load_pickle(read_to_node_dict_path)

    ## group read headers by unitig ID
    utg_reads = defaultdict(list)
    for idx, vals in iter_badreads_fastq(inpath) if badreads else iter_pbsim_maf(inpath):
        utg_reads[r2n[idx]].append(vals)

    ## set values for each unitig
    utg_vals = dict()
    for idx, headers in utg_reads:
        # check CHR and strand is matching
        assert reduce(lambda x, y: x == y, h['chr'] for h in headers)
        assert reduce(lambda x, y: x == y, h['strand'] for h in headers)

        # start is the earliest, end is the latest of all reads mapping to a utig
        start = min(h['start'] for h in headers)
        end = max(h['end'] for h in headers)

        zygosity = check_zygosity(h[0]['chr'], start, end)

        utg_vals[idx] = {'chr': h[0]['chr'], 'strand': h[0]['strand'], 'start': start, 'end': end, 'zygosity': zygosity}

    ## then add to graph and save to output
    nx.set_node_attributes(graph, utg_vals)

    save_pickle(graph, outpath)


if __name__ == '__main__':
    import argparse as ap
    parser = ap.ArgumentParser(description="This is a small tool to attach ground truth values to a hic-augmented unitig graph, used for generating a training dataset for grapHiC")

    parser.add_argument("-i", "--input-graph", dest='in_graph', required=True, type=ap.FileType('r'), help="Input unitig graph, in pickled networkx format.")
    parser.add_argument("-f", "--input-fastq", dest='in_fastq', default='-', type=ap.FileType('r'), help="Input FASTQ file. Default stdin. Should be generated by badreads, and have headers containing a ground truth for each read.")
    parser.add_argument("--pbsim", dest='pbsim', action='store_true', default=False, help="Specify a PBSim3 instead of a badreads input FASTQ. NOT YET IMPLEMENTED.")
    parser.add_argument("-o", dest='outfile', default='-', type=ap.FileType('w'), help="Where to write the output networkx graph. Default stdout.")
    parser.add_argument("-d", dest='dictfile', required=True, type=ap.FileType('r'), help="File containing the Read ID to Utig ID mapping as a pickled python dict.")

    args = parser.parse_args()
    graph = get_connection_graph(args.infile.name, args.dictfile.name)
